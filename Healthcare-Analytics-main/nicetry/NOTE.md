项目分为四个阶段
- 分析数据、清理数据
- 特征数据处理
- 构建模型、训练模型
- 预测结果、结果分析


## 分析数据、清理数据
`warnings.filterwarnings('ignore')` 的作用是忽略警告信息。在 Python 中，有些函数或操作可能会生成警告信息，这些警告信息通常用于提醒开发者潜在的问题或不推荐的做法。通过调用 `warnings.filterwarnings('ignore')`，你可以暂时忽略所有警告，使其不会在程序运行过程中显示。

`.head()`用于查看前几行数据

`.info()`用于查看表信息

`.isnull().sum().sort_values(ascending=False)`用于查看缺省数据

`.shape` 用于查看表结构

`[i].nunique()`用于返回列的唯一值总数

## 特征数据处理

使用`LabelEncoder`来编码非数值类型的值

通过建模，提取已知信息中有价值的部分

在这里，'case_id', 'patientid', 'Hospital_region_code', 'Ward_Facility_Code' 等单独来看都没有太大的作用，但是可以统计一个人的就医次数（count_id_patient），一个人到某一个医院的次数

划分数据集

使用了 `sklearn` 库中的 `train_test_split` 函数，将数据集 `X1` 和目标变量 `y1` 分割成训练集和测试集。具体参数的含义如下：

- `X1`：特征数据集。
- `y1`：目标变量数据集。
- `test_size`：测试集所占比例，这里设置为 0.20，表示将数据集分割成 80% 的训练集和 20% 的测试集。
- `random_state`：随机种子，用于控制数据集的随机分割。设置了随机种子可以保证每次运行得到的随机结果是相同的，方便调试和复现结果。

函数的返回值是四个数据集：
- `X_train`：训练集特征数据。
- `X_test`：测试集特征数据。
- `y_train`：训练集目标变量数据。
- `y_test`：测试集目标变量数据。

## 构建模型、训练模型

### GaussianNB
这段代码使用了 `sklearn` 库中的朴素贝叶斯算法中的高斯朴素贝叶斯模型（`GaussianNB`）。具体步骤如下：

1. `target = y_train.values`：将训练集的目标变量 `y_train` 转换为 NumPy 数组形式，赋值给 `target`。

2. `features = X_train.values`：将训练集的特征数据 `X_train` 转换为 NumPy 数组形式，赋值给 `features`。

3. `classifier_nb = GaussianNB()`：创建一个高斯朴素贝叶斯分类器的实例。

4. `model_nb = classifier_nb.fit(features, target)`：使用训练集的特征数据 `features` 和目标变量数据 `target` 来训练高斯朴素贝叶斯模型，得到训练好的模型 `model_nb`。

#### 原理

高斯朴素贝叶斯（Gaussian Naive Bayes）是朴素贝叶斯（Naive Bayes）算法的一种。朴素贝叶斯算法是一种基于贝叶斯定理和特征条件独立假设的分类算法，适用于分类问题。

高斯朴素贝叶斯模型假设特征变量（即输入变量）的概率分布服从高斯分布（也称为正态分布）。因此，在训练阶段，该模型会计算每个类别下每个特征的均值和方差，以便后续在预测阶段使用这些统计量来估计新样本的类别。

高斯朴素贝叶斯模型的工作原理可以简要描述如下：

1. 对每个类别计算每个特征的均值和方差。
2. 对于新的样本，计算其在每个类别下每个特征的概率密度函数值（根据高斯分布计算）。
3. 根据贝叶斯定理和条件独立性假设，计算新样本属于每个类别的概率。
4. 将新样本分配给具有最高概率的类别。

需要注意的是，朴素贝叶斯算法中的“朴素”指的是特征条件独立性假设，即假设每个特征在给定类别下是独立的。尽管这个假设在现实中并不总是成立，但在许多实际问题中，朴素贝叶斯模型仍然表现良好，并且具有快速的训练和预测速度。

### XGBoost

这段代码使用了 XGBoost（Extreme Gradient Boosting）库，创建了一个 XGBoost 分类器 `XGBClassifier` 的实例，并设置了一些参数。具体参数的含义如下：

- `max_depth=4`：决策树的最大深度，用于控制树模型的复杂度，防止过拟合。
- `learning_rate=0.1`：学习率，控制每次迭代中模型参数更新的幅度。
- `n_estimators=800`：要构建的树的数量（迭代次数）。
- `objective='multi:softmax'`：目标函数，这里指定为多分类问题的 softmax 目标函数。
- `reg_alpha=0.5`：L1 正则化项（Lasso）的权重，用于控制模型的复杂度。
- `reg_lambda=1.5`：L2 正则化项（Ridge）的权重，用于控制模型的复杂度。
- `booster='gbtree'`：指定使用的 booster 类型，这里是基于树的提升模型。
- `n_jobs=4`：并行运行的作业数，用于加速模型的训练过程。
- `min_child_weight=2`：叶子节点的最小样本权重和，用于控制树的生长。
- `base_score=0.75`：基础分数，模型在初始迭代时的预测分数。


XGBoost（Extreme Gradient Boosting）是一种基于决策树集成的机器学习算法，它在梯度提升框架下实现了高效的、可扩展的梯度提升算法。XGBoost 通过在每一轮迭代中训练新的模型来逐步改进先前模型的预测结果，从而构建一个强大的集成模型。

#### 原理
XGBoost 的工作原理可以概括如下：

1. **构建基本模型（基学习器）：** 首先，使用训练数据构建一个初始的基本模型（比如决策树）作为第一个学习器。

2. **计算残差：** 通过将当前模型的预测结果与实际标签进行比较，计算出残差（预测值与实际值之间的差异），作为下一轮模型的训练目标。

3. **构建新模型：** 使用残差作为新的标签，构建一个新的模型，该模型会尝试纠正前一个模型的错误。

4. **更新模型权重：** 通过将新模型的预测结果与前一个模型的预测结果相加，得到一个更好的预测结果。然后更新模型的权重，使得在下一轮迭代中更多地考虑新模型的预测结果。

5. **重复迭代：** 重复以上步骤，每一轮迭代都会构建一个新的模型，直到达到预先设定的迭代次数或达到停止条件为止。

XGBoost 还通过正则化技术（如 L1 和 L2 正则化）来控制模型的复杂度，防止过拟合。它还使用了特征重要性评估来帮助理解模型的预测过程。总体上，XGBoost 通过不断地改进模型的预测能力，从而提高了模型的性能和泛化能力。

#### 参数的选择

max_depth：决策树的最大深度，控制树的复杂度。通常选择一个适当的值来防止过拟合。

learning_rate：学习率，控制每次迭代中模型参数的更新幅度。较小的学习率可以使模型更加稳定，但可能需要更多的迭代次数。

n_estimators：要构建的树的数量（迭代次数），即基学习器的数量。

subsample：训练每棵树时使用的子样本的比例。可以用来防止过拟合。

colsample_bytree：每棵树在训练时随机选择特征的比例。

gamma：节点分裂所需的最小损失减少量。可以用来控制模型的复杂度。

reg_alpha：L1 正则化项的权重，用于控制模型的复杂度。

reg_lambda：L2 正则化项的权重，用于控制模型的复杂度。

scale_pos_weight：正负样本的权重平衡。在类别不平衡的情况下，可以设置为正样本数目/负样本数目。

objective：损失函数的类型。对于分类问题，通常使用 binary:logistic（二分类）或 multi:softmax（多分类）。

### nn

这段代码使用了 Keras 中的 `to_categorical` 函数，将类别标签（即 `y_train` 和 `y_test`）转换为 one-hot 编码的形式。one-hot 编码是一种常见的用于处理分类问题标签的编码方式，它将每个类别表示为一个向量，向量的长度等于类别的总数，其中只有一个元素为1，其余元素为0，这个1表示样本所属的类别。

在这个例子中，`to_categorical(y_train)` 和 `to_categorical(y_test)` 将训练集和测试集的类别标签转换为 one-hot 编码的形式，分别存储在变量 `a` 和 `b` 中。这样处理后，模型可以更好地理解类别之间的关系，从而提高模型的性能。

这段代码使用了 Keras 库构建了一个简单的深度神经网络模型，用于多分类问题。下面是对代码中各部分的解释：

1. `Sequential()`：创建了一个序列模型，可以将各层按顺序堆叠在一起。

2. `model.add(tf.keras.Input(shape=(20,)))`：添加了一个输入层，指定输入数据的形状为 `(20,)`，表示输入特征的数量为20。

3. `model.add(Dense(64, activation='relu'))`：添加了一个全连接层，包含64个神经元，并使用 ReLU 激活函数。

4. `model.add(Dense(128, activation='relu'))`：添加了一个包含128个神经元的全连接层，并使用 ReLU 激活函数。

5. `model.add(Dense(256, activation='relu'))`：添加了一个包含256个神经元的全连接层，并使用 ReLU 激活函数。

6. `model.add(Dense(512, activation='relu'))`：添加了一个包含512个神经元的全连接层，并使用 ReLU 激活函数。

7. `model.add(Dense(512, activation='relu'))`：再次添加了一个包含512个神经元的全连接层，并使用 ReLU 激活函数。

8. `model.add(Dense(11, activation='softmax'))`：添加了一个输出层，包含11个神经元（假设有11个类别），并使用 softmax 激活函数，用于多分类问题的输出。

9. `model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])`：编译模型，指定优化器为 SGD（随机梯度下降），损失函数为分类交叉熵（categorical_crossentropy），评估指标为准确率（accuracy）。

这样构建的模型具有多个隐藏层，每个隐藏层使用 ReLU 激活函数，输出层使用 softmax 激活函数，适用于多分类问题。您可以根据实际情况调整模型结构和参数，以获得更好的性能。

这段代码使用了 TensorBoard 回调函数来监控模型的训练过程，并将日志数据保存到指定目录（"logs_keras"）中，以便后续使用 TensorBoard 进行可视化分析。

`model.fit` 方法用于训练模型，其中的参数含义如下：
- `X_train`：训练集特征数据。
- `a`：经过 one-hot 编码的训练集目标变量。
- `epochs=20`：训练的轮数，即迭代次数。
- `callbacks=callbacks`：指定回调函数，这里使用了 TensorBoard 回调函数。
- `validation_split=0.2`：将训练集的 20% 作为验证集用于模型验证。

在训练过程中，TensorBoard 会不断地保存模型的训练指标（如损失和准确率）到日志文件中，您可以使用 TensorBoard 工具来加载这些日志文件，并进行模型性能的可视化分析。
